{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8353b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96183f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90efea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #re: 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba22477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 텍스트 분해 과정\n",
    "\n",
    "# 아무 문장이나 뉴스에서 긁어와서 텍스트로 담아줌.\n",
    "text = \"손흥민은 5일 영국 런던의 셀허스트 파크에서 열린 2022~2023시즌 잉글랜드 프리미어리그(EPL) 19라운드에 선발 출전해 리그 4호골을 터뜨리며 크리스털 팰리스를 상대로 4-0 대승을 이끌었다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de99320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['손흥민은',\n",
       " '5일',\n",
       " '영국',\n",
       " '런던의',\n",
       " '셀허스트',\n",
       " '파크에서',\n",
       " '열린',\n",
       " '2022~2023시즌',\n",
       " '잉글랜드',\n",
       " '프리미어리그(EPL)',\n",
       " '19라운드에',\n",
       " '선발',\n",
       " '출전해',\n",
       " '리그',\n",
       " '4호골을',\n",
       " '터뜨리며',\n",
       " '크리스털',\n",
       " '팰리스를',\n",
       " '상대로',\n",
       " '4-0',\n",
       " '대승을',\n",
       " '이끌었다.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split() # 빈공간을 기준으로 문장 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d459d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '나는', '뽀로로야']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아무 문장이나 더 연습\n",
    "text2 = \"안녕+나는+뽀로로야\"\n",
    "re.split(\"\\+\", text2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee222b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '나는', '뽀로로야']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2.split(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028e40a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '5678', '3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = \"\"\"\n",
    "이름:크롱\n",
    "전화번호:010-1234-5678\n",
    "나이:3\n",
    "성별:남아\n",
    "\"\"\"\n",
    "re.findall(\"\\d+\", text3) # d: 숫자만 추출해서 뽑아내고 싶을 경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad2aad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                     EPL                                 '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4 = \"그 효과는 득점 장면을 제외한 경기 내용에서도 잘 드러났다. 손흥민이 중앙으로 이동할 때마다 경기가 손쉽게 풀렸다. 후반 22분 단짝인 케인이 찔러준 공을 받은 뒤 질주해 페널티지역에서 슈팅을 시도한 것이 대표적이다. 1분 뒤에는 케인에서 건네받은 공을 맷 도허티에게 연결해 추가골로 이어졌다. EPL 규정상 공식 어시스트로 기록되지 않은 게 아쉬울 따름이다.\"\n",
    "re_res = re.sub(\"[^a-zA-Z]\", ' ', text4) # 영어 소문자나 대문자가 아닌것은 ' '로 가져와서 이외의 문자들 모두 지워버림\n",
    "re_res\n",
    "\n",
    "# 영문자만 남기고 한글, 숫자 다 사라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad097b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EPL']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_res.split() # 추출한 영문자를 공백으로 나눠서 단어 단위로 나눠줌\n",
    "\n",
    "# 앞으로 단어들을 추출하고자 할 때 이런 방식으로 많이 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5c0f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅋㅋㅋso?ㅠㅠ'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원하는 문자만 다 찾아서 지워보자.\n",
    "\n",
    "text5 = \"ㅋㅋㅋ so 이따가 시간돼? ㅠㅠ\"\n",
    "re.compile(\"[ |가-힣]+\").sub(\"\", text5) # (\"[ |가-힣]+\")이 조건에 맞는 문자는 없애라\n",
    "\n",
    "# `가-힣` : 단순 자음, 모음을 제외한 한글 문자의 모든 범위. 영문자로 치면 약 `a-Z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da1cc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이따가', '시간돼']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text5 = \"ㅋㅋㅋ so 이따가 시간돼?\"\n",
    "re.compile(\"[가-힣]+\").findall(text5) # (\"[가-힣]+\") 이 조건에서 중요한 문자(단어)만 추출해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee177b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅋㅋㅋ', 'so', '?', 'ㅠㅠ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text6 = \"ㅋㅋㅋ so 이따가 시간돼? ㅠㅠ\"\n",
    "re.compile(\"[가-힣]+\").sub(\"\", text6).split() # (\"[ |가-힣]+\")이 조건에 맞는 문자는 없애라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b2a3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://naver.me/5CpLf8TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b3f7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlpy 설치는 자바가 필요함\n",
    "# 자바 jdk -> pip install jpype -> pip install konlpy\n",
    "# 64bit와 파있너 버전을 확인하여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "410db527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09ff34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff2a3afa-8bfd-47f5-b9a4-7b7462455623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2583d602-2c72-4975-998f-460d366f5057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '자연어', '처리', '를', '배우고', '있어요', ',', '너무', '신기해요']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt #트위터에서 한글용으로 만든 패키지\n",
    "okt = Okt()\n",
    "tokens = okt.morphs(\"나는 자연어 처리를 배우고 있어요, 너무 신기해요\") # 토큰에 처리하고자 하는 문자를 넣어줌\n",
    "tokens\n",
    "\n",
    "# 영어도 형태소 분석기를 써야함.\n",
    "# 한글도 조사가 너무 많이 붙어서 형태소 분석기를 거의 필수로 써야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb872eee-b70c-444c-9c30-236ad02b6719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나는', '자연어', '처리를', '배우고', '있어요,', '너무', '신기해요']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"나는 자연어 처리를 배우고 있어요, 너무 신기해요\".split() # 단어 단위로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08f4c2b4-ec11-46ab-8853-06765544cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어전용 형태소 분석기 설치\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a4f2562-e12c-4034-9710-861c937d9c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc751f87-87dd-4ece-959d-cddc5facb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc4979d2-8f71-4927-afd7-de15ae50bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"According to local legend, the devil escaped from hell and created the jagged glacial karsts in a single day of mischief.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c681648-7d32-4630-a49e-c6bf199f48f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According', 'to', 'local', 'legend', ',', 'the', 'devil', 'escaped', 'from', 'hell', 'and', 'created', 'the', 'jagged', 'glacial', 'karsts', 'in', 'a', 'single', 'day', 'of', 'mischief', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15f2d67c-f131-402c-9851-250ba2dde20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According', 'to', 'local', 'legend', ',', 'the', 'devil', 'escaped', 'from', 'hell', 'and', 'created', 'the', 'jagged', 'glacial', 'karsts', 'in', 'a', 'single', 'day', 'of', 'mischief', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70c46f75-d0dc-4802-a9ed-48b8e9b3fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['according', 'to', 'local', 'legend', 'the', 'devil', 'escaped', 'from', 'hell', 'and', 'created', 'the', 'jagged', 'glacial', 'karsts', 'in', 'a', 'single', 'day', 'of', 'mischief']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(text)) # 위와 다르게 쉼표가 사라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cd0198d-06c0-441e-9691-bd5170d34545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According to local legend, the devil escaped from hell and created the jagged glacial karsts in a single day of mischief.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 문장의 텍스트를 문장 단위로 토큰화 할 때\n",
    "\n",
    "# 데이터 수집의 경우 - 단어 단위로 데이터를 수집\n",
    "# 챗봇같은 경우에 활용할 땐 - 문장 단위로 데이터를 수집\n",
    "\n",
    "text = \"\"\"According to local legend, the devil escaped from hell and created the jagged glacial karsts in a single day of mischief.\n",
    "\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5524fc87-8d9d-464d-a91b-add2b0905433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 한국어 문장 분리 형태소분석기 설치\n",
    "# !pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fed86119-b698-40b5-942b-e76705cf8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d69d5a4-8469-498f-9692-c902149974b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['딥러닝 자연어 처리는 흥미롭습니다.', '그런데 재미는 없을 수도 있습니다.', '특히 일상 언어는 너무 복잡합니다.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"딥러닝 자연어 처리는 흥미롭습니다. 그런데 재미는 없을 수도 있습니다. 특히 일상 언어는 너무 복잡합니다.\"\n",
    "kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5a75c97-641f-4b20-a65a-b38daf298ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 사용시에는 아래와 같은 문제를 고려해봐야 한다.\n",
    "\n",
    "# 1. 원하는 파싱(Parsing) 결과가 나오는가?\n",
    "# 2. 처리 속도는 적절한가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e6e1729-1fdc-461d-8455-7840ab62284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e3e86f5-b71e-416f-8abe-c569a0399550",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b79c43d8-cd42-4da7-9f64-2c6478c67c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- OKT ----\n",
      "형태소 분석 ['열심히', '코딩', '한', '당신', ',', '잠도', '잘', '자고', '일', '하세요', '.']\n",
      "품사 태깅 [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('잠도', 'Noun'), ('잘', 'Verb'), ('자고', 'Noun'), ('일', 'Noun'), ('하세요', 'Verb'), ('.', 'Punctuation')]\n",
      "명사 분석 ['코딩', '당신', '잠도', '자고', '일']\n",
      "---- KKMA ----\n",
      "형태소 분석 ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '잠', '도', '잘', '자', '고', '일하', '세요', '.']\n",
      "품사 태깅 [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('잠', 'NNG'), ('도', 'JX'), ('잘', 'MAG'), ('자', 'VV'), ('고', 'ECE'), ('일하', 'VV'), ('세요', 'EFN'), ('.', 'SF')]\n",
      "명사 분석 ['코딩', '당신', '잠']\n"
     ]
    }
   ],
   "source": [
    "text = \"열심히 코딩한 당신, 잠도 잘 자고 일하세요.\"\n",
    "print('---- OKT ----')\n",
    "print(\"형태소 분석\", okt.morphs(text))\n",
    "print(\"품사 태깅\", okt.pos(text)) \n",
    "print(\"명사 분석\", okt.nouns(text)) #단어를 쪼갤 때는 명사를 주로 분석한다.print('---- OKT ----')\n",
    "print('---- KKMA ----')\n",
    "print(\"형태소 분석\", kkma.morphs(text))\n",
    "print(\"품사 태깅\", kkma.pos(text)) \n",
    "print(\"명사 분석\", kkma.nouns(text)) #단어를 쪼갤 때는 명사를 주로 분석한다.\n",
    "\n",
    "# 구두를 추천받아야 하는 상황에 드레스를 추천하면 하면 안되기 때문에 구분 잘 하는게 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65eedce7-f042-4a9a-bbac-25427bdf9fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 깃에서 라이브러리 가져옴.\n",
    "# 파이썬 / 띄어쓰기를 자동으로 해주는 라이브러리\n",
    "# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3ddb6eb-cb3f-49af-a898-7d6fe68fba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"'그 효과는 득점 장면을 제외한 경기 내용에서도 잘 드러났다. 손흥민이 중앙으로 이동할 때마다 경기가 손쉽게 풀렸다. 후반 22분 단짝인 케인이 찔러준 공을 받은 뒤 질주해 페널티지역에서 슈팅을 시도한 것이 대표적이다. 1분 뒤에는 케인에서 건네받은 공을 맷 도허티에게 연결해 추가골로 이어졌다. EPL 규정상 공식 어시스트로 기록되지 않은 게 아쉬울 따름이다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9512ca6-dc81-49bd-8b42-9dca4d0a9f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'그효과는득점장면을제외한경기내용에서도잘드러났다.손흥민이중앙으로이동할때마다경기가손쉽게풀렸다.후반22분단짝인케인이찔러준공을받은뒤질주해페널티지역에서슈팅을시도한것이대표적이다.1분뒤에는케인에서건네받은공을맷도허티에게연결해추가골로이어졌다.EPL규정상공식어시스트로기록되지않은게아쉬울따름이다.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 띄어쓰기 전부 삭제 해주기\n",
    "new_text = text.replace(\" \", \"\")\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7ece927-ee95-468a-910b-82bee1ad089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import Spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b5ade36-beb0-4360-8928-5bf2f5418ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 412ms/step\n",
      "'그 효과는 득점 장면을 제외한 경기 내용에서도 잘 드러났다. 손흥민이 중앙으로 이동할 때마다 경기가 손쉽게 풀렸다. 후반 22분 단짝인 케인이 찔러준 공을 받은 뒤 질주해 페널티지역에서 슈팅을 시도한 것이 대표적이다. 1분 뒤에는 케인에서 건네받은 공을 맷 도허티에게 연결해 추가골로 이어졌다. EPL 규정상 공식 어시스트로 기록되지 않은 게 아쉬울 따름이다.\n",
      "'그 효과는 득점 장면을 제외한 경기내용에서도 잘 드러났다. 손흥민이 중앙으로 이동할 때마다 경기가 손쉽게 풀렸다. 후반 22분 단짝 인케인이 찔러준공을 받은 뒤 질주해 페널티 지역에서 슈팅을 시도한 것이 대표적이다.1분 뒤에는 케인에서 건네받은 공을 맷도 허티에게 연결해 추가골로 이어졌다. EPL 규정상 공식 어시스트로 기록되지 않은 게 아쉬울 따름이다.\n"
     ]
    }
   ],
   "source": [
    "# 원본 문장이랑 자동 띄어쓰기 처리 해준 내용 비교해보기\n",
    "s = Spacing()\n",
    "recon_text = s(new_text)\n",
    "print(text)\n",
    "print(recon_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfa71fc5-c20d-453b-acb0-4c129aa7c3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 맞춤법 교정 라이브러리\n",
    "# !pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f38b7d05-9559-4baa-822e-b1a5d267ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6952513-aaf8-4307-8dc4-291e379ad521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맞춤법 틀리면 외않된데? 내맴대로 쓰묜되징!'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일부러 맞춤법 틀린 문장 적기\n",
    "text_x = \"마춤법 틀리묜 외않된데? 내맴대로 쓰묜되징!\"\n",
    "text_ok = spell_checker.check(text_x)\n",
    "text_ok.checked\n",
    "\n",
    "# 심각한 문장은 잘 못고친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a7001a3-9f00-4e1c-8a77-d6adb02b1ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맞춤법 틀리면 왜 안돼? 내 맘대로 쓰면 되지!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일부러 맞춤법 틀린 문장 적기\n",
    "text_x = \"마춤법 틀리면 외 않되? 내맘대로 쓰면돼지!\"\n",
    "text_ok = spell_checker.check(text_x)\n",
    "text_ok.checked\n",
    "\n",
    "# 맞춤법이 적당히 틀리면 교정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ac9d254-ebea-4408-9bb4-36a581f4b851",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 반복문자 간소화 라이브러리\n",
    "# !pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5fd3578f-51d1-4caa-afd6-acf68be6d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ca97f07-b70a-4702-8622-3acaa64d76cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아ㅋㅋ김밥존맛탱쿠쿠쿠ㅜㅜㅜㅋㅋ'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이김밥존맛탱쿠쿠쿠ㅜㅜㅜㅋㅋㅋㅋㅋㅋㅋㅋ'\n",
    "emoticon_normalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f561d253-b4bf-4bc1-bf64-3247873b16cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아ㅋㅋㅋ김밥존맛탱쿠쿠쿠ㅜㅜㅜㅋㅋㅋ'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoticon_normalize(text, num_repeats=3) # num_repeats : 길이 조절도 직접 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e204d34-99c4-4376-85c4-5eacedf6ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어, Stopwords 라이브러리 : 쓰지않는 용어, 조사처럼 불필요한 글자 빼고 싶을 때 사용\n",
    "import nltk\n",
    "# 필요한 패키지 미리 다운로드\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c28a86c1-28e9-4d0d-9cf1-64957f7e4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec354f60-bb24-41ef-8a31-7a541d2517c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 몇개 정도의 불용어가 정의돼있는지 확인\n",
    "stop_words_list = stopwords.words(\"english\")\n",
    "len(stop_words_list)\n",
    "\n",
    "# 179개의 불용어 정의됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8c35c49-4c9a-4321-8c92-e1f2cb333180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떤 내용을 불용어로 정의했는지 10개만 확인\n",
    "stop_words_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6145300f-767a-4b26-a1b7-64a3ef9836f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', 'jassy', 'did', 'not', 'specify', 'where', 'affected', 'employees', 'were', 'located', ',', 'but', 'he', 'said', 'the', 'firm', 'would', 'communicate', 'with', 'organisations', 'that', 'represent', 'employees']\n"
     ]
    }
   ],
   "source": [
    "test = \"Mr Jassy did not specify where affected employees were located, but he said the firm would communicate with organisations that represent employees\"\n",
    "test = test.lower() # 소문자로 변환\n",
    "token_1 = word_tokenize(test) # 워드 토크나이저로 분석해보기. (불용어 제거 전)\n",
    "print(token_1) # 문장 내 단어 모두 분리.\n",
    "# for문으로 불용어가 아닌것만 빈리스트에 넣어주는 방법으로도 필터링 작업 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "89ee7ec1-3317-45b6-8461-3f8d1720dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', 'jassy', 'specify', 'affected', 'employees', 'located', ',', 'said', 'firm', 'would', 'communicate', 'organisations', 'represent', 'employees']\n"
     ]
    }
   ],
   "source": [
    "token_2 = [] # 불용어가 아닌것만 담을 빈 리스트 정의. 필터링 된 것만 담아줌.\n",
    "for word in token_1:\n",
    "    if word not in stop_words_list:\n",
    "        token_2.append(word)\n",
    "print(token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa85936d-3871-4213-9bd9-d7ce5d680bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'i' in stop_words_list # 특정 용어가 불용어 목록에 있는지 조회해보는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f93dd62-55d3-4690-8ddc-059eb9e0c1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'he' in stop_words_list # 특정 용어가 불용어 목록에 있는지 조회해보는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64c96e6b-aa67-4f7f-a3f4-84a7a43afa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'fuck' in stop_words_list # 특정 용어가 불용어 목록에 있는지 조회해보는 법\n",
    "# 이렇게 추가 해야할 것 같은, 추가하고싶은 불용어 단어가 있다면\n",
    "# 새로운 리스트 만들어서 추가해서 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61e20531-0fe9-4718-8a9e-9031d3fd0613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drop', 'your', 'weapon', '.', 'fuck', 'you', 'ass', 'hole', '.', 'go', 'the', 'hell', 'mother', 'fucker']\n",
      "['drop', 'weapon', '.', 'fuck', 'ass', 'hole', '.', 'go', 'hell', 'mother', 'fucker']\n",
      "['drop', 'weapon', '.', '.', 'go', 'hell', 'mother', 'fucker']\n"
     ]
    }
   ],
   "source": [
    "# 직접 설정한 불용어까지 필터링 해보는 작업\n",
    "test = \"Drop your weapon. Fuck you ass hole. go the hell mother fucker\"\n",
    "test = test.lower() # 소문자로 변환\n",
    "token_1 = word_tokenize(test) # 워드 토크나이저로 분석해보기. (불용어 제거 전)\n",
    "print(token_1) # 문장 내 단어 분리.\n",
    "\n",
    "# 기본 제공 불용어 필터링 해주고\n",
    "token_2 = [] # 불용어가 아닌것만 담을 빈 리스트 정의. 필터링 된 것만 담아줌.\n",
    "for word in token_1:\n",
    "    if word not in stop_words_list:\n",
    "        token_2.append(word)\n",
    "print(token_2)\n",
    "\n",
    "my_stop_words = [\"fuck\", \"ass\", \"hole\"]\n",
    "\n",
    "# 내가 따로 거르고자 하는 불용어 한번 더 걸러줌\n",
    "token_3 = []\n",
    "for word in token_2:\n",
    "    if word not in my_stop_words:\n",
    "        token_3.append(word)\n",
    "print(token_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aff599d0-d1a7-4582-ab56-24fa2226ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '따위', '물건', '을', '팔고', '도', '돈', '을', '쳐', '먹냐', '그냥', '줘도', '아깝다', '.', '하자', '있는', '물건', '을', '어떻게', '쓰냐', '.', '병신', '아']\n"
     ]
    }
   ],
   "source": [
    "# morphs\n",
    "okt = Okt()\n",
    "text = \"이 따위 물건을 팔고도 돈을 쳐먹냐 그냥 줘도 아깝다. 하자 있는 물건을 어떻게 쓰냐. 병신아\"\n",
    "word_token = okt.morphs(text)\n",
    "print(word_token)\n",
    "\n",
    "# 한글 처리 할 때는 조사와 같은 한글자 짜리 문자를 모두 빼버린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae36af28-34a6-43c6-830f-35c43c1b0e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"이\"), len(\"물건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0332bf87-a0b6-4b97-a1c4-19544c900b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['따위', '물건', '팔고', '먹냐', '그냥', '줘도', '아깝다', '하자', '있는', '물건', '어떻게', '쓰냐', '병신']\n"
     ]
    }
   ],
   "source": [
    "token_ko_2 = []\n",
    "for word in word_token:\n",
    "    if len(word) > 1: # 조사와 지시대명사 다 날려버림\n",
    "        token_ko_2.append(word)\n",
    "print(token_ko_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36158b7c-50d0-46c1-9246-a760fcf5203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['따위', '물건', '팔고', '먹냐', '그냥', '줘도', '아깝다', '하자', '있는', '물건', '어떻게', '쓰냐']\n"
     ]
    }
   ],
   "source": [
    "# 거르고자하는 용어 리스트 생성\n",
    "my_stop_words_ko = [\"fuck\", \"ass\", \"hole\", \"이\", \"병신\", \"ㅄ\", \"ㅂㅅ\", \"벼ㅇ신\"]\n",
    "\n",
    "# 내가 따로 거르고자 하는 불용어 한번 더 걸러줌\n",
    "token_ko_3 = []\n",
    "for word in token_ko_2:\n",
    "    if word not in my_stop_words_ko:\n",
    "        token_ko_3.append(word)\n",
    "print(token_ko_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfc0c766-07bd-4123-9042-5e05bfde6ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '따위', '물건', '팔고', '돈', '그냥', '하자', '물건', '병신']\n"
     ]
    }
   ],
   "source": [
    "word_token_nouns = okt.nouns(text)\n",
    "print(word_token_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "77b16e1c-b533-496f-9a6a-b459508ded70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['따위', '물건', '팔고', '돈', '그냥', '하자', '물건']\n"
     ]
    }
   ],
   "source": [
    "# 내가 따로 거르고자 하는 불용어 한번 더 걸러줌\n",
    "token_ko_3 = []\n",
    "for word in word_token_nouns:\n",
    "    if word not in my_stop_words_ko:\n",
    "        token_ko_3.append(word)\n",
    "print(token_ko_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8472b-4bf0-4cd4-8dd5-de3e6ec0723b",
   "metadata": {},
   "source": [
    "* ASCII( /ˈæski/, 아스키)는 영문 알파벳을 사용하는 대표적인 문자 인코딩이다. 대부분의 문자 인코딩이 아스키에 기초를 두고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92e0df00-91fc-4be9-96d5-d7fa4a5d8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 : 컴퓨터는 10진법으로 텍스트를 표현하기 때문에 정수 인코딩으로 문자를 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee4c4cee-3f4b-4b2d-8351-57c036f42953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복되는 문자 추출\n",
    "# 단어 수가 많지 않고 반복이 있는 가사 선정.\n",
    "\n",
    "text = \"\"\"오늘 네가\n",
    "보고싶다\n",
    "널 다시 품에\n",
    "안아보고 싶다\n",
    "오늘 네가 난\n",
    "생각난다\n",
    "너랑 같이 산책하던\n",
    "그곳에 서있다\n",
    "너의 체온이\n",
    "기억난다\n",
    "따뜻하게 내 쉬던\n",
    "숨소리 들려\n",
    "오늘 네가\n",
    "온 것 같아\n",
    "우리 같이 잠들던\n",
    "벤치에 기대니\n",
    "시간이 흘러흘러\n",
    "다시 만날 순 없지\n",
    "그래도 보고싶다 널\n",
    "그리운 내 사랑아\n",
    "오늘 네가 정말\n",
    "보고싶다\n",
    "너를 다시 내 품에\n",
    "안아보고 싶다\n",
    "오늘 네가 난\n",
    "생각난다\n",
    "너를 쓰다듬던 손이\n",
    "너를 기억한다\n",
    "니가 보고싶다\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "41ae4a65-6416-4dc6-9707-7a216a046bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 한글 쪼개기 (토큰화)\n",
    "okt = Okt()\n",
    "text = text.replace(\"\\n\", \" \") # 데이터 정제\n",
    "text1 = okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e894857-1dc3-47e4-9646-624ca1249c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '다시', '안아', '보고', '싶다', '오늘', '같이', '산책', '하던', '서있다', '체온', '쉬던', '숨소리', '들려', '오늘', '같아', '우리', '같이', '잠들던', '벤치', '기대니', '시간', '흘러', '흘러', '다시', '만날', '없지', '그래도', '그리운', '사랑', '오늘', '정말', '다시', '안아', '보고', '싶다', '오늘', '기억']\n"
     ]
    }
   ],
   "source": [
    "# 두 글자 이상의 문자들을 선별해서 담아줌\n",
    "text2 = []\n",
    "for word in text1:\n",
    "    if 4 > len(word) > 1: # 조사와 지시대명사 다 날려버림\n",
    "        text2.append(word)\n",
    "print(text2)\n",
    "\n",
    "# 조건에 맞는 단어들 다 리스트에 쌓아둠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c396965-2e64-4b5b-91e7-8e8c91b7f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 27)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text2), len(set(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7fbae64f-7533-40b1-bc66-5b3e66d4b24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'오늘': 5,\n",
       " '다시': 3,\n",
       " '안아': 2,\n",
       " '보고': 2,\n",
       " '싶다': 2,\n",
       " '같이': 2,\n",
       " '산책': 1,\n",
       " '하던': 1,\n",
       " '서있다': 1,\n",
       " '체온': 1,\n",
       " '쉬던': 1,\n",
       " '숨소리': 1,\n",
       " '들려': 1,\n",
       " '같아': 1,\n",
       " '우리': 1,\n",
       " '잠들던': 1,\n",
       " '벤치': 1,\n",
       " '기대니': 1,\n",
       " '시간': 1,\n",
       " '흘러': 2,\n",
       " '만날': 1,\n",
       " '없지': 1,\n",
       " '그래도': 1,\n",
       " '그리운': 1,\n",
       " '사랑': 1,\n",
       " '정말': 1,\n",
       " '기억': 1}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 키를 추가해줌\n",
    "vocab = {}\n",
    "for word in text2: # for 문을 돌리면서 처음 만난 단어라면 \n",
    "    if word not in vocab: \n",
    "        vocab[word] = 0 # vocab 이라는 딕셔너리에 key와 value값 넣어준다\n",
    "    vocab[word] += 1\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dfce06f-20cc-4756-88c7-b9be4fc8d8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('오늘', 5), ('다시', 3), ('안아', 2), ('보고', 2), ('싶다', 2), ('같이', 2), ('산책', 1), ('하던', 1), ('서있다', 1), ('체온', 1), ('쉬던', 1), ('숨소리', 1), ('들려', 1), ('같아', 1), ('우리', 1), ('잠들던', 1), ('벤치', 1), ('기대니', 1), ('시간', 1), ('흘러', 2), ('만날', 1), ('없지', 1), ('그래도', 1), ('그리운', 1), ('사랑', 1), ('정말', 1), ('기억', 1)])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c95effb-0444-436f-a41f-1393aff1c1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 5),\n",
       " ('다시', 3),\n",
       " ('안아', 2),\n",
       " ('보고', 2),\n",
       " ('싶다', 2),\n",
       " ('같이', 2),\n",
       " ('흘러', 2),\n",
       " ('산책', 1),\n",
       " ('하던', 1),\n",
       " ('서있다', 1),\n",
       " ('체온', 1),\n",
       " ('쉬던', 1),\n",
       " ('숨소리', 1),\n",
       " ('들려', 1),\n",
       " ('같아', 1),\n",
       " ('우리', 1),\n",
       " ('잠들던', 1),\n",
       " ('벤치', 1),\n",
       " ('기대니', 1),\n",
       " ('시간', 1),\n",
       " ('만날', 1),\n",
       " ('없지', 1),\n",
       " ('그래도', 1),\n",
       " ('그리운', 1),\n",
       " ('사랑', 1),\n",
       " ('정말', 1),\n",
       " ('기억', 1)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정렬의 기준을 정한다\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True)\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a78ec49-c3d2-43a1-8c84-c67593d6d967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'오늘': 1, '다시': 2, '안아': 3, '보고': 4, '싶다': 5, '같이': 6, '흘러': 7}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수가 높은 단어만 선별 (2개 이상)\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, freq) in vocab_sorted: # 워드, 빈도수가 vocab_sorted 여기에서 높은거 추출\n",
    "    if freq > 1: # 2번 이상 반복되는 것들을 순서대로 레이블 인코딩\n",
    "        i += 1\n",
    "        word_to_index[word] = i\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d99e2c92-da94-4859-b47d-4397b5886e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n ('오늘', 5),\\n ('다시', 3),\\n ('안아', 2),\\n ('보고', 2),\\n ('싶다', 2),\\n ('같이', 2),\\n ('흘러', 2),\\n\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `말뭉치(Corpus)` : 언어학에서 (여러 언어의) 텍스트 집합\n",
    "# 여기까지의 과정이 아래와 같은 말뭉치를 직접 만들어 보는 과정\n",
    "\"\"\"\n",
    " ('오늘', 5),\n",
    " ('다시', 3),\n",
    " ('안아', 2),\n",
    " ('보고', 2),\n",
    " ('싶다', 2),\n",
    " ('같이', 2),\n",
    " ('흘러', 2),\n",
    "\"\"\"\n",
    "\n",
    "# [작업 순서]\n",
    "# 1. 데이터 선정\n",
    "# 2. 데이터 토큰화\n",
    "# 3. 데이터 빈도 수를 따져서 말뭉치 생성\n",
    "# 주의 할 점 : 말뭉치를 인코딩할 때는 숫자 `0`은 보통 인코딩에 포함 시키지 않는다. \n",
    "# 나중에 딥러닝 패딩 작업할 때 사용해야하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246e0f9-d03c-4f63-9518-98a24ce66299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
